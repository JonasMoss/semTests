% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utility.R
\name{distance}
\alias{distance}
\title{Estimate the distance between a sample and the uniform distribution.}
\usage{
distance(
  x,
  dist = c("kolmogorov-smirnov", "anderson-darling", "kullback-leibler",
    "cramer-von mises", "0.05-distance")
)
}
\arguments{
\item{x}{a numeric vector of observations in \verb{[0,1]}.}

\item{dist}{a distance measure.}
}
\value{
Estimated distance between the distribution of \code{x} and the uniform
distribution.
}
\description{
Estimate the distance between a sample and the uniform distribution using
a statistical distance measure.
}
\details{
The option \code{kullback-leibler} uses the Vasicek estimator of the differential
entropy, implemented in \code{vsgoftest.} The option \verb{cramer-von mises}
returns the Cramer-von Mises distance, the option \code{anderson-darling}
returns the Anderson-Darling distance (implemented in \code{goftest}), and
the \code{kolmogorov-smirnov} option return the Kolmogorov-Smirmov distance. The
option \code{0.05-distance} measures the distances between the observed
proportion below \code{0.05} and \code{0.05} itself.
}
\references{
Anderson, T.W. and Darling, D.A. (1954) A test of goodness of fit.
Journal of the American Statistical Association 49, 765-69.

Cramer, H. (1928). "On the Composition of Elementary Errors".
Scandinavian Actuarial Journal. 1928 (1): 13-4.

Vasicek, O., A test for normality based on sample entropy,
Journal of the Royal Statistical Society, 38(1), 54-59 (1976).

Daniel, Wayne W. (1990). "Kolmogorov-Smirnov one-sample test".
Applied Nonparametric Statistics (2nd ed.). Boston: PWS-Kent. pp. 319-30.
}
